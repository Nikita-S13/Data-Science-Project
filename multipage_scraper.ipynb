{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "42a6f9f3d74aa4028d9613a039ad99fa2c45f012ef34c6ad76a472252c749f23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "            return soup\n",
    "        else:\n",
    "            print('failed')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect(soup,container):\n",
    "    if soup:\n",
    "        names = soup.find_all('div',attrs={'class':'_4rR01T'})\n",
    "        prices = soup.find_all('div',attrs={'class':'_30jeq3 _1_WHN1'})\n",
    "        ratings= soup.find_all('span',attrs={'class':'_2_R_DZ'})\n",
    "        try:\n",
    "            url = soup.find_all('a',attrs={'class':'_1fQZEK'})[1]\n",
    "        except:\n",
    "            url = soup.find('a',attrs={'class':'_1fQZEK'})\n",
    "        for nm,pr,rt in zip(names,prices,ratings):\n",
    "            item = {'product':nm.text,\n",
    "                    'price':pr.text,\n",
    "                    'ratings':rt.text}\n",
    "            container.append(item)\n",
    "        if url:\n",
    "            url = url.attrs.get('href')\n",
    "            curl = \"https://www.flipkart.com\"+url\n",
    "            print('next page link ==>',curl)\n",
    "            return curl,container\n",
    "        else:\n",
    "            print('no next url found')\n",
    "            return None,container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(filename,datalist):\n",
    "    data= pd.DataFrame(datalist)\n",
    "    data.to_csv(filename)\n",
    "    print('saved to',filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5a4c25c2f87e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnewurl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnewurl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'the end'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "container = []\n",
    "url = \"https://www.flipkart.com/search?q=mobiles\"\n",
    "while True:\n",
    "    soup = get(url)\n",
    "    newurl,container = collect(soup,container)\n",
    "    if not newurl:\n",
    "        print('the end')\n",
    "        break\n",
    "    else:\n",
    "        url = newurl\n",
    "save(filename='flipkart.csv',datalist=container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}